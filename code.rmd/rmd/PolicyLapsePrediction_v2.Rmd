---
title: "Policy Lapse Prediction with Tidymodels"
author: "Tam Pham"
date: "`r Sys.Date()`"
output: 
  html_document: 
    theme: flatly
    toc: true
    toc_float: true
    toc_depth: 2
    code_folding: show
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE,
                      message = FALSE, echo = TRUE, dpi = 180,
                      fig.width = 8, fig.height = 5,
                      digits = 3)

library(tidyverse) # manipulating data
library(tidymodels)
```

## Explore Data

Our modeling goal is to predict the policy Lapsed or In-force based on the Policy information, Customer demography and interaction frequency to policy events.

```{r include=FALSE}
policy <- readxl::read_xlsx(here::here( "data", "PolicyLapse.xlsx"))

policy <- 
  policy %>%
  janitor::remove_empty("cols") %>%
  select(-c("PaymentTerm0","DistributionChannel0","PolicyYear")) %>%
  mutate_if(is.character,factor) %>%
  mutate(Lapsed = fct_rev(Lapsed))
```

**Summary policy data:**

```{r skim, echo=FALSE}
library(skimr)
my_skim <- skim_with(factor=sfl(pct = ~{
      prt <- sort(prop.table(table(.)), decreasing = TRUE)
      val <- sprintf("%.2f", prt)
      nm1 <- substr(names(prt), 1, 3)
      stringr::str_c(nm1, val, sep = ": ", collapse = ", ")
      })
)
my_skim(policy)
```

The EDA steps have been done in another document. [Check here for EDA](http://rpubs.com/pmtam/PolicyLapseEDA).

## Feature engineering

### Split data and resample

The first step of our analysis is to split data into two separate sets: "training" set and "testing" set. The "training" set will be used to train the model while the "testing" set to evaluate the final model.

In order to prevent over fitting during training, we will resample training data using *k-fold cross validated* with `vfold-cv()`. With k=10 fold data set, we expect better estimate of the model's performance.

```{r}
set.seed(123)
policy_split <- policy %>%
  drop_na() %>%
  initial_split(prop = 0.80, strata = Lapsed)
train <- training(policy_split)
test <- testing(policy_split)

# Cross validation folds (default v=10)
folds <- vfold_cv(train, strata = Lapsed,
                  repeats = 3)
```

### Pre-processing

Before adding our data to the model, we need to pre-process our data, using `recipe()`:

```{r}
pol_rec <-
  recipe(Lapsed ~., data= train) %>%
  update_role(ID, new_role = "id pol") %>% # change ID variable as 'id role'
  step_corr(all_numeric()) %>% # filter for High Correlation for numeric data.
  step_dummy(all_nominal(), -all_outcomes()) %>% # Convert nominal data to dummy variable,
  step_zv(all_numeric()) # filter zero variance
#  step_normalize(all_numeric_predictors()) # We might not need for Tree-based methods
pol_rec 
```

<!-- - Tree-based algorithms can handle any kind of numeric data and might not required number to be normalized (scaled and center). -->
<!-- - xgboost manage only numeric vector, so all nominal data should be convert to dummy  -->

## Build models

### Type of models

We will build the models based on Tree-based methods: Decision Tree, Bagging `bag_tree()`, Random Forest `rand_forest()` and Boosting `boost_tree()`. 
<br>
Other models can be found here <https://www.tidymodels.org/find/parsnip/>

### Model Examples {.tabset}
#### Decision Tree
##### Specify model

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>% 
  set_mode("classification")
```

##### Creat Workflow

```{r}
tree_wf <- workflow() %>%
  add_recipe(pol_rec) %>%
  add_model(tree_spec)
```

##### Fit the model

```{r}
set.seed(123)
#future::plan(multisession)

tree_fit <- fit_resamples(
    tree_wf,
    resamples = folds,
    metrics = metric_set(accuracy,roc_auc,sens, spec),
    control = control_resamples(save_pred = TRUE))
```

#### Bagged Trees
##### Specify model

```{r}
library(baguette)
bag_spec <- bag_tree() %>%
  set_engine("rpart", times = 10) %>% # 10 boostrap resample 
  set_mode("classification")
```

##### Creat Workflow
```{r}
bag_wf <- workflow() %>%
  add_recipe(pol_rec) %>%
  add_model(bag_spec)
```

##### Fit the model
```{r}
set.seed(123)
#future::plan(multisession)

bag_fit <- fit_resamples(
    bag_wf,
    resamples = folds,
    metrics = metric_set(accuracy,roc_auc,sens, spec),
    control = control_resamples(verbose = TRUE,
                                save_pred = TRUE))
```

#### Random Forest
##### Specify model

```{r}
rf_spec <- rand_forest() %>%
  set_engine("ranger",  importance = "impurity",
             verbose = TRUE) %>%
  set_mode ("classification") %>%
  set_args(trees = 1000)
```

##### Creat Workflow

```{r}
rf_wf <- workflow() %>%
  add_recipe(pol_rec) %>%
  add_model(rf_spec)
```

##### Fit the model

```{r}
set.seed(123)
#future::plan(multisession)

rf_fit <- fit_resamples(
    rf_wf,
    resamples = folds,
    metrics = metric_set(accuracy,roc_auc,sens, spec),
    control = control_resamples(save_pred = TRUE))
```

#### Boosted Trees
##### Specify model

```{r}
xgb_spec <-
  boost_tree(
#    trees = tune(),
#    mtry = tune(),
#    min_n = tune(),
#    learn_rate = 0.01
  ) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
```

##### Creat Workflow
```{r}
xgb_wf <- workflow() %>%
  add_recipe(pol_rec) %>%
  add_model(xgb_spec)
```

##### Fit the model
```{r}
set.seed(123)
#future::plan(multisession)
xgb_fit <- fit_resamples(
    xgb_wf,
    resamples = folds,
    metrics = metric_set(accuracy,roc_auc,sens, spec),
    control = control_resamples(save_pred = TRUE))
```

## Evaluate models

Four models have been build. We will evaluate models performance by comparing their metrics:

###  {.tabset}

#### The metrics

After running these four methods, now it's time to evaluate their performance with `collect_metric()` function. Below are the table for comparison :

```{r class.source = 'fold-hide'}
options(digits = 3)
tree_fit %>% collect_metrics() %>%
  select(.metric,mean) %>%
  rename("tree" = "mean") %>%
  bind_cols(collect_metrics(bag_fit) %>%
            select(mean) %>%
              rename("bag" = mean)) %>%
  bind_cols(collect_metrics(rf_fit) %>%
            select(mean) %>%
              rename("rf" = "mean")) %>%
  bind_cols(collect_metrics(xgb_fit) %>%
            select(mean) %>%
              rename("xgb" = mean)
            ) %>%
  knitr::kable(caption = "Metric evaluation")
```

```{r echo=FALSE}
library(tidytext)
tree_fit %>% collect_metrics() %>%
  mutate(model = "tree") %>%
  bind_rows(bag_fit %>%
              collect_metrics() %>%
              mutate(model = "bag")) %>%
  bind_rows(rf_fit %>%
              collect_metrics() %>%
              mutate(model = "rf")) %>%
  bind_rows(xgb_fit %>%
              collect_metrics() %>%
              mutate(model = "xgb")) %>%
  mutate(.metric = as.factor(.metric),
         model1=reorder_within(model, mean, .metric)) %>%
  ggplot(aes(model1, mean, fill=model))+
  geom_col(alpha=0.8, position = "dodge", show.legend = FALSE)+
  geom_text(aes(label=sprintf("%1.3f", mean)),
            position=position_dodge(width=0.9), vjust=1.5)+
  labs(x=NULL,title = "Metrics comparision",
       subtitle = "with training data set") +
  facet_wrap(~.metric, scales = "free_x")+
  scale_x_reordered()
```

Look likes:

-   **Random forest** did better on roc_auc and overall accuracy. It also has the highest specificity but has lowest sensitivity.

-   **Decision tree** also did good on overall accuracy and have highest sensitivity.

#### ROC Curve

## Finalize model

The final step is to fit the trained model to testing data using `last_fit()` function.

```{r}
fn_metrics <- metric_set(roc_auc, accuracy, sens, spec)
# Decision Tree
tree_final <- last_fit(
  tree_wf,
  split = policy_split,
  metrics = fn_metrics
)
# Bagging Trees
bag_final <- last_fit(
  bag_wf,
  split = policy_split,
  metrics = fn_metrics
)
# Random forest
rf_final <- last_fit(
  rf_wf,
  split = policy_split,
  metrics = fn_metrics
)
# boosted tree
xgb_final <- last_fit(
  xgb_wf,
  split = policy_split,
  metrics = fn_metrics
)
```

The table below shows the actual out-of-sample performance for each of our four models.

```{r echo=FALSE}
collect_metrics(tree_final) %>% mutate(model="tree") %>%
  rbind(
    collect_metrics(bag_final) %>% mutate(model="bag")
  ) %>%
  rbind(
    collect_metrics(rf_final) %>% mutate(model="rf")
  ) %>%
  rbind(
    collect_metrics(xgb_final) %>% mutate(model="xgb")
  ) %>%
   mutate(.metric = as.factor(.metric),
         model1=reorder_within(model, .estimate, .metric)) %>%
  ggplot(aes(model1, .estimate, fill=model))+
  geom_col(alpha=0.8, position = "dodge", show.legend = FALSE)+
  geom_text(aes(label=sprintf("%1.3f", .estimate)),
            position=position_dodge(width=0.9), vjust=1.5)+
  labs(x=NULL,title = "Metrics comparision", subtitle = "with testing data set") +
  facet_wrap(~.metric, scales = "free_x")+
  scale_x_reordered()
```

```{r include = FALSE, echo= FALSE}
tree_final %>% collect_metrics() %>%
  select(.metric,.estimate) %>%
  rename("tree" = .estimate) %>%
  bind_cols(collect_metrics(bag_final) %>%
            select(.estimate) %>%
              rename("bag" = .estimate)) %>%
  bind_cols(collect_metrics(rf_final) %>%
            select(.estimate) %>%
              rename("rf" = .estimate)) %>%
  bind_cols(collect_metrics(xgb_final) %>%
            select(.estimate) %>%
              rename("xgb" = .estimate)
            ) %>%
  knitr::kable(caption = "Metric evaluation")
```

After applying four trained models to the unseen test data, similar to the perfomance metrics with resampling training dataset, it looks like:

-   Random Forests did better on overall accuracy and roc_auc. But it has the lowest sensitivity rate. If *consider accuracy/roc_auc as selection criteria*, then the **Random Forests** is our finalized model.

-   If *the objective is to identify the likelihood of lapse*, we should consider the model with the highest Sensitivity rate. In this case, **Decision Tree** is the selected model:

    -   **The selected model - Decision Tree** reaches an accuracy of **`r round(collect_metrics(tree_final)[1,3],3)`** . It means, the model can help us to classify correctly 8 out of 10 times whether the policy Lapsed or Inforce.

    -   The sensitivity rate **`r round(sensitivity(collect_predictions(tree_final),Lapsed,.pred_class)[,3],3)`** , a bit higher than estimate on training data, tells us 7.7 out of 10 lapsed policy can be predicted by model.

A closer look on the predictions with Decision Tree:

###  {.tabset}

#### Confusion matrix

***Confusion matrix with Decision Tree***

```{r}
tree_final %>% collect_predictions() %>%
  conf_mat(Lapsed,.pred_class)
```


#### Variable importance

Plot variable importance scores for the predictors in the model

```{r vip_plot, class.source='fold-hide'}
library(vip)
tree_final %>%
  extract_fit_engine() %>% # extract engine from the final model
  vip() + ggtitle("Tree")
```

#### Prediction

The following table is the Prediction rate of Lapsed/In-force, comparing to their Truth status:

```{r class.source = 'fold-hide'}
# predicting with testing data
library(DT)
tree_final %>% collect_predictions() %>%
  select(.pred_Lapsed,.pred_Inforce,.pred_class,Lapsed) %>%
  rename("Truth" = Lapsed,
         "Predicted" = .pred_class) %>%
  mutate(PolicyID = test$ID, .before=1) %>%
    arrange(desc(.pred_Lapsed)) %>%
  datatable(caption = "Prediction rate of Lapsed/Inforce", filter = "top") %>%
  formatRound(2:3,digits = 3)
```

From this table, we can filter the policy which is currently In-force *(Truth="Inforce")* but predicting Lapse *(Predicted="Lapse")* by the model, we can also view the Lapse probability rate for those policies.

### 

#### Bonus: Plot the selected Decision Tree

```{r echo=FALSE}
tree_final %>%
  extract_fit_engine() %>%
  rpart.plot::rpart.plot(type = 4, extra = 2,
                         roundint = FALSE)
```


