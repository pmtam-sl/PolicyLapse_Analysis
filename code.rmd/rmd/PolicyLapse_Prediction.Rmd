---
title: "Policy Lapse Prediction"
author: "Tam Pham"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float: true
    toc_depth: 2
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE,
                      message = FALSE, echo = TRUE, dpi = 180,
                      fig.width = 8, fig.height = 5,
                      digits = 3)
library(tidyverse)
library(tidymodels)
```

## Explore Data

Our modeling goal is to predict the policy Lapsed or Inforce based on the Policy information, Customer demography and the interaction to policy events.

Let's take a closer look to policy data:

```{r include=FALSE}
policy <-
readxl::read_xlsx("~/OneDrive/Learning/R for Data Science/Insurance_practices/data/PolicyLapse.xlsx")

policy <- 
  policy %>%
  janitor::remove_empty("cols") %>%
#  select(!where(is.logical),-c("PaymentTerm0","DistributionChannel0","PolicyYear")) %>%
  select(-c("PaymentTerm0","DistributionChannel0","PolicyYear")) %>%
  mutate_if(is.character,factor) %>%
  mutate(Lapsed = fct_rev(Lapsed))
```

**Summary policy data:**

```{r skim, echo=FALSE}
library(skimr)
my_skim <- skim_with(factor=sfl(pct = ~{
      prt <- sort(prop.table(table(.)), decreasing = TRUE)
      val <- sprintf("%.2f", prt)
      nm1 <- substr(names(prt), 1, 3)
      stringr::str_c(nm1, val, sep = ": ", collapse = ", ")
      })
)
my_skim(policy)
```

The EDA steps have been done in another document.[Check here for EDA](http://rpubs.com/pmtam/PolicyLapseEDA).

<br>
Let's take straightforward to the prediction steps.

## Feature engineering

### Split data and resample

The first step of our analysis is to split data into two separate sets: a "training" set and a "testing" set. Then resample training data using vfold-cv()

```{r}
set.seed(1234)
policy_split <- policy %>%
  na.omit() %>%
  initial_split(prop = 0.80, strata = Lapsed)
policy_train <- training(policy_split)
policy_test <- testing(policy_split)

set.seed(234)
# Cross validation folds (default cv=10), with 3 repeats.
folds <- vfold_cv(policy_train, strata = Lapsed, 
                  repeats = 3)
```

### Pre-processing

Before adding our data to model, we need to pre-process, using recipe():

```{r}
# Create recipe
pol_rec <-
  recipe(Lapsed ~., data= policy_train) %>%
  update_role(ID, new_role = "id pol") %>% # change ID variable as 'id role'
  step_corr(all_numeric()) %>% # filter for High Correlation for numeric data.
  step_dummy(all_nominal(), -all_outcomes()) %>% # Convert nominal data to dummy variable, but not convert out_comes variable
  # missing handling - by median imputation
  # step_impute_median(PO_Age) %>%
  step_zv(all_numeric()) %>% # filter zero variance
#  step_normalize(all_numeric(),-ID) # normalize numeric data to have SD of one and mean of zero (Center and scale)
  step_normalize(all_numeric_predictors())
# pol_rec 

# It still did not do anything as this just a setup then we run it again with prep() command:
pol_prep <- pol_rec %>%  
  prep()
pol_prep
```

```{r juice, include=FALSE}
# juice() for checking how look like the data after pre-processing
juice(pol_prep)
# bake() to apply the pre-processing operations that has been training by prep()
bake(pol_prep, new_data = policy_train) # apply pre-processing as we did for training.
bake(pol_prep, new_data = NULL) # we don't assign new_data as we just want to check with policy_train data.
```

```{r include=FALSE, echo=FALSE}
# We can create resampling k-fold after reprocessing
#set.seed(345)
#folds <- vfold_cv(juice(pol_prep), strata = Lapsed) # we can create k-fold with reciped data
#folds
```

## Build Models

### Model specification

We will build the models based on the following algorithms: Logistic regression, k-nearest neighbor, Decision Tree and Random forest. Let's create model specification:

```{r Model Specification}
set.seed(123)
# Logistic regression
glm_spec <- logistic_reg() %>%
  set_engine('glm')# declare a computation engine to fit to model

# Nearest Neighbor (knn)
knn_spec <- nearest_neighbor() %>%
  set_engine("kknn") %>% 
  set_mode("classification")

# Decision tree
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>% 
  set_mode("classification")

# Random Forest
rf_spec <- rand_forest(trees = 1000) %>%
  set_engine("ranger",  importance = "impurity") %>%
  set_mode ("classification")
```

```{r include=FALSE}
# xgboost
xgb_spec <-
  boost_tree(
    trees = tune(),
    mtry = tune(),
    min_n = tune(),
    learn_rate = 0.01
  ) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

set.seed(100)
mod_bag <- bag_tree() %>%
  set_mode("regression") %>%
  set_engine("rpart", times = 10) # 10 bootstrap resamples
```

#### Fit model spec to Dataset

Fitting Glm model specification to the pre-processed training data set:

```{r}
# glm
glm_fit <- glm_spec %>%
  fit(Lapsed ~., data = juice(pol_prep))
glm_fit
```

```{r include=FALSE}
# glm_fit model has been build. We can "tidy" the model to select which variable has the p.value less than .05
tidy(glm_fit) %>%  # turn object (model) into a tidy tibble
  filter(p.value < 0.05)
```

Fitting other model specification : knn, tree and rf

```{r}
knn_fit <- knn_spec %>%
  fit(Lapsed ~., data = juice(pol_prep))

tree_fit <- tree_spec %>%
  fit(Lapsed ~., data = juice(pol_prep))

rf_fit <- rf_spec %>%
  fit(Lapsed ~., data = juice(pol_prep))
```

### Create Workflow

```{r}
# Main workflow is created 
pol_wf <- workflow() %>%
  add_recipe(pol_rec)
```

### Modeling with Resampling data

We are now fitting the models to resamples. Fitting Glm model, and display 10 folds with 3 repeats:

```{r}
doParallel::registerDoParallel() # do Parallel for multi-cores
set.seed(456)
glm_rs <- pol_wf %>%
  add_model(glm_spec) %>% # using spec of glm to fit again the resampling
  fit_resamples(
    resamples = folds, # fit for 10 folds cv
    metrics = metric_set(accuracy,roc_auc,sens, spec),
    control = control_resamples(save_pred = TRUE))
glm_rs
```

The performance of model can be viewed via `.metrics` and `.predictions`

```{r}
# display the metric (accuracy,roc_auc,sens, spec) for each fold
glm_rs %>%
  unnest(.metrics) 
glm_rs %>%
  unnest(.predictions)
```

Continue fitting other models to resamples

```{r}
set.seed(456)
tree_rs <- pol_wf %>%
  add_model(tree_spec) %>%
  #tree_spec %>% 
  fit_resamples(
    resamples = folds,
    metrics = metric_set(accuracy,roc_auc,sens, spec),
    control = control_resamples(save_pred = TRUE))

set.seed(456)
knn_rs <- pol_wf %>%
  add_model(knn_spec) %>% 
  fit_resamples(
    resamples = folds,
    metrics = metric_set(accuracy,roc_auc,sens, spec),
    control = control_resamples(save_pred = TRUE))

set.seed(456)
rf_rs <- pol_wf %>%
  add_model(rf_spec) %>%
  fit_resamples(
    resamples = folds,
    metrics = metric_set(accuracy,roc_auc,sens, spec),
    control = control_resamples(save_pred = TRUE))
```

```{r include=FALSE, echo=FALSE}
# Used k-folds with reprocessing data
set.seed(456)
glm_rs <- glm_spec %>% # using spec of glm to fit again the resampling
  fit_resamples(Lapsed ~.,
                resamples = folds, # fit for 10 folds cv
                metrics = metric_set(accuracy,roc_auc,sens, spec),
                control = control_resamples(save_pred = TRUE))
glm_rs
glm_rs %>%
  unnest(.metrics) # display the metric (sens, spec, roc_auc) for each fold

glm_rs %>%
  unnest(.predictions) 

set.seed(456)
tree_rs <- 
  tree_spec %>%
  fit_resamples(Lapsed ~.,
                resamples = folds, # fit for 10 folds cv
                metrics = metric_set(accuracy,roc_auc,sens, spec),
                control = control_resamples(save_pred = TRUE))

set.seed(456)
knn_rs <- knn_spec %>% 
  fit_resamples(Lapsed ~ .,
                resamples = folds,
                metrics = metric_set(accuracy,roc_auc,sens, spec),
                control = control_resamples(save_pred = TRUE))

set.seed(456)
rf_rs <- rf_spec %>%
  fit_resamples(Lapsed ~.,
    resamples = folds,
    metrics = metric_set(roc_auc, accuracy, sens, spec),
    control = control_resamples(save_pred = TRUE)
  )
```

## Evaluate models

### Evaluate metrics from models

Four models have been build. We will evaluate models performance by comparing their metrics:

```{r include=FALSE}
# Compare metrics from 4 models after resampling
glm_rs %>% collect_metrics()
tree_rs %>% collect_metrics()
knn_rs %>% collect_metrics()
rf_rs %>% collect_metrics()
```

```{r class.source = 'fold-hide'}
options(digits = 3)
glm_rs %>% collect_metrics() %>%
  select(.metric,mean) %>%
  rename("glm" = "mean") %>%
  bind_cols(collect_metrics(tree_rs) %>%
            select(mean) %>%
              rename("tree" = "mean")
  ) %>%
  bind_cols(collect_metrics(rf_rs) %>%
            select(mean) %>%
              rename("rf" = "mean")
  ) %>%
  bind_cols(collect_metrics(knn_rs) %>%
            select(mean) %>%
              rename("knn" = "mean")
  ) %>%
  knitr::kable(caption = "Metric evaluation")
```

```{r echo=FALSE}
library(tidytext)
glm_rs %>% collect_metrics() %>%
  mutate(model = "glm") %>%
  bind_rows(tree_rs %>%
              collect_metrics() %>%
              mutate(model = "tree")) %>%
  bind_rows(knn_rs %>%
              collect_metrics() %>%
              mutate(model = "knn")) %>%
  bind_rows(rf_rs %>%
              collect_metrics() %>%
              mutate(model = "rf")) %>%
  mutate(.metric = as.factor(.metric),
         model1=reorder_within(model, mean, .metric)) %>%
  ggplot(aes(model1, mean, fill=model))+
  geom_col(alpha=0.8, position = "dodge", show.legend = FALSE)+
  geom_text(aes(label=sprintf("%1.3f", mean)),
            position=position_dodge(width=0.9), vjust=1.5)+
  labs(x=NULL,title = "Metrics comparision") +
  facet_wrap(~.metric, scales = "free_x")+
  scale_x_reordered()
```

Look likes:

-   **Random forest** did better on roc_auc and overall accuracy. It also has the highest specificity.

-   **Decision tree** also did good on overall accuracy and have highest sensitivity.

#### ROC curve

```{r echo = FALSE}
# Grouping all prediction into one table
glm_rs %>%
  unnest(.predictions) %>%
  mutate(model = "glm") %>%
  bind_rows(tree_rs %>%
              unnest(.predictions) %>%
              mutate(model = "tree")) %>%
  bind_rows(knn_rs %>%
              unnest(.predictions) %>%
              mutate(model = "knn")) %>%
  bind_rows(rf_rs %>%
              unnest(.predictions) %>%
              mutate(model = "rf")) %>%
  group_by(model) %>%
  roc_curve(Lapsed,.pred_Lapsed) %>%
  autoplot()
```

#### ROC curve for each model

```{r include=FALSE}
glm_roc <-
glm_rs %>%
  collect_predictions() %>% 
  group_by(id2) %>%
  roc_curve(Lapsed, .pred_Lapsed) %>%
  autoplot()
```

```{r include=FALSE}
tree_roc <-
tree_rs %>%
  # Obtain results
  collect_predictions() %>% 
  group_by(id2) %>%
  # made ROC curve with the read truth "Lapsed" and the predicted probability
  roc_curve(Lapsed, .pred_Lapsed) %>%
  # we got tibble with 10 group of specificity and sensitivity. 
  autoplot()
```

```{r include=FALSE}
rf_roc <-
rf_rs %>%
  collect_predictions() %>% 
  group_by(id2) %>%
  roc_curve(Lapsed, .pred_Lapsed) %>%
  autoplot()
```

```{r include=FALSE}
knn_roc <-
knn_rs %>%
  collect_predictions() %>% 
  group_by(id2) %>%
  roc_curve(Lapsed, .pred_Lapsed) %>%
  autoplot()
```

```{r class.source = 'fold-hide' }
ggpubr::ggarrange(glm_roc, tree_roc, rf_roc, knn_roc,
                  labels = c("GLM", "Tree", "RF", "Knn"),
                  nrow = 2, ncol = 2)
```

#### Confusion matrix

From the comparison, we can ignore knn algorithm. We then try to find the people who Lapsed, by checking confusion matrix:

####  {.tabset}

##### Tree

```{r echo=FALSE}
conf_mat_resampled(tree_rs)
```

Policy which did lapse, and were predicted to lapse (freq=`r round(conf_mat_resampled(tree_rs)[1,3],0)`) is about twice as Lapsed policies which were predicted wrongly (`r round(conf_mat_resampled(tree_rs)[3,3],0)`). So about 2/3 of policy which is lapsed were predicted correctly. Then why sensitivity is **`r round(collect_metrics(tree_rs)[3,3],3)`**

The specificity tells us the policy which still in-force were predicted correctly `r round(collect_metrics(tree_rs)[4,3],3)`.

##### RF

```{r echo=FALSE}
conf_mat_resampled(rf_rs)
```

Lapsed detection (or sensitive) rate by Random forest is **`r round(collect_metrics(rf_rs)[3,3],3)`**

##### GLM

```{r echo=FALSE}
conf_mat_resampled(glm_rs)
```

Lapsed detection (or sensitive) rate by GLM is **`r round(collect_metrics(glm_rs)[3,3],3)`**

#### 

As objective to predict the policy which is lapsed, it seems *decision tree* did better.

## Finalize model

Fit the final model to the training set and evaluate the test set. Then we can select the final model

###  {.tabset}

#### Tree - the selected model

```{r tree_final}
tree_final <- pol_wf %>% # take the workflow of work
  # add the model specification agin
  add_model(tree_spec) %>%
  # apply with the last fit
  last_fit(policy_split) # with main members split
```

We did fit on the training set and evaluate on test set in order to have the final predictions.

We will compare metrics from testing data to metric on resampling on training:

```{r}
collect_metrics(tree_final) # metrics from final tree model with testing data
collect_metrics(tree_rs) # metrics on resampling on training
```

The accuracy from testing is just a little bit better comparing to resampling data set.

*View prediction*

```{r confusion matrix, class.source = 'fold-hide'}
collect_predictions(tree_final) %>%
  conf_mat(Lapsed, .pred_class)
```

This looks pretty good: the model doing a pretty good job of recognizing both Lapsed and Inforce policy. We then consider **Decision Tree as the selected model.**

#### RF final

```{r include=FALSE}
rf_final <- pol_wf %>%
  add_model(rf_spec) %>%
  last_fit(policy_split)
```

```{r}
# Performance of RF model on testing
collect_metrics(rf_final)
# Compare to resamples on training
collect_metrics(rf_rs) 
```

```{r}
collect_predictions(rf_final) %>%
  conf_mat(Lapsed, .pred_class)
```

#### GLM final

```{r include=FALSE}
# glm
glm_final <- pol_wf %>%
  add_model(glm_spec) %>%
  last_fit(policy_split)
```

```{r}
# Performance of GLM model on testing
collect_metrics(glm_final)
# Compare to resamples on training
collect_metrics(glm_rs)
```

```{r }
collect_predictions(glm_final) %>%
  conf_mat(Lapsed, .pred_class)
```

### Variable importance

Plot variable importance scores for the predictors in the model

```{r vip_plot, class.source='fold-hide'}
library(vip)
vip_tree <-
  tree_final %>%
  extract_fit_engine() %>% # extract engine from the final model
  vip() + ggtitle("Tree")

vip_rf <-
  rf_final %>%
  extract_fit_engine() %>% # extract engine from the final model
  vip() + ggtitle("RF")

vip_glm <-
  glm_final %>%
  extract_fit_engine() %>% # extract engine from the final model
  vip() + ggtitle("GLM")

ggpubr::ggarrange(vip_tree, vip_rf, vip_glm,
                  ncol = 3, nrow = 1) 
```

```{r include=FALSE, echo=FALSE}
# Variable importance plots in this fit object
tree_final %>%
  extract_fit_engine() %>%
  vip(geom = "point", num_features = 12) # we are plotting 12 variables base in their importance scores in the model
```

*Premium, NumOfReinstated, NumOfCalls, NumberOfEmails, Occupation* play some important score in classifying Lapsed and Inforce.

## Conclusion and Prediction

-   **The selected model - Decision Tree** reaches an accuracy of **`r round(collect_metrics(tree_final)[1,3],3)`**. It means, the model can help us to classify correctly 8 out of 10 times whether the policy Lapsed or Inforce.

-   The sensitivity rate (\~`r round(sensitivity(collect_predictions(tree_final),Lapsed,.pred_class)[,3],3)` ) tells us 6 out of 10 lapsed policy can be predicted by model.

-   The following table is the Prediction rate of Lapsed/Inforce, comparing to their Truth status:

```{r class.source = 'fold-hide'}
# predicting with testing data
library(DT)
tree_fit %>%
  predict(new_data = bake(pol_prep, new_data = policy_test),
          type = "prob") %>%
  mutate(PolicyID = policy_test$ID, .before = 1) %>%
  mutate(Truth = policy_test$Lapsed) %>%
#  filter(Truth == "Lapsed") %>%
  arrange(desc(.pred_Lapsed)) %>%
  datatable(caption = "Prediction rate of Lapsed/Inforce", filter = "top") %>%
  formatRound(2:3,digits = 3)
```

```{r include=FALSE, echo=FALSE}
# predicting with testing data
glm_fit %>%
  predict(new_data = bake(pol_prep, new_data = policy_test),
          type = "prob") %>%
  mutate(PolicyID = policy_test$ID, .before = 1) %>%
  mutate(Truth = policy_test$Lapsed)

glm_fit %>%
  predict(new_data = bake(pol_prep, new_data = policy_test),
          type = "prob") %>%
  mutate(Truth = policy_test$Lapsed) %>%
  roc_auc(Truth, .pred_Lapsed)
```

```{r include=FALSE, echo=FALSE}
# predicting with testing data
tree_fit %>%
  predict(new_data = bake(pol_prep, new_data = policy_test),
          type = "prob") %>%
  mutate(PolicyID = policy_test$ID, .before = 1) %>%
  mutate(Truth = policy_test$Lapsed)

tree_fit %>%
  predict(new_data = bake(pol_prep, new_data = policy_test),
          type = "prob") %>%
  mutate(Truth = policy_test$Lapsed) %>%
  roc_auc(Truth, .pred_Lapsed)
```

```{r include=FALSE, echo=FALSE}
# predicting with testing data
rf_fit %>%
  predict(new_data = bake(pol_prep, new_data = policy_test),
          type = "prob") %>%
  mutate(PolicyID = policy_test$ID, .before = 1) %>%
  mutate(Truth = policy_test$Lapsed)

rf_fit %>%
  predict(new_data = bake(pol_prep, new_data = policy_test),
          type = "prob") %>%
  mutate(Truth = policy_test$Lapsed) %>%
  roc_auc(Truth, .pred_Lapsed)
```

### Bonus: Plot the selected decision tree

```{r include=FALSE, echo=FALSE}
tree_fit$fit %>%
  rpart.plot::rpart.plot(type = 4, extra = 2,
                         roundint = FALSE)
```

```{r echo=FALSE}
tree_final %>%
  extract_fit_engine() %>%
  rpart.plot::rpart.plot(type = 4, extra = 2,
                         roundint = FALSE)
```


```{r include=FALSE, echo=FALSE}
# Fitting testing dataset.
tree_test <-
augment(tree_fit, 
        new_data = bake(pol_prep, new_data = policy_test))

tree_test %>%
  accuracy(truth = Lapsed, estimate = .pred_class)

tree_test %>%
  conf_mat(truth = Lapsed, estimate = .pred_class)
```

